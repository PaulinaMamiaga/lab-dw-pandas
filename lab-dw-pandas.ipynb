{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d7736c-ba17-4aff-b6bb-66eba20fbf4e",
   "metadata": {},
   "source": [
    "# Lab | Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1973e9e-8be6-4039-b70e-d73ee0d94c99",
   "metadata": {},
   "source": [
    "In this lab, we will be working with the customer data from an insurance company, which can be found in the CSV file located at the following link: https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv\n",
    "\n",
    "The data includes information such as customer ID, state, gender, education, income, and other variables that can be used to perform various analyses.\n",
    "\n",
    "Throughout the lab, we will be using the pandas library in Python to manipulate and analyze the data. Pandas is a powerful library that provides various data manipulation and analysis tools, including the ability to load and manipulate data from a variety of sources, including CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045146f-f4f7-44d9-8cd9-130d6400c73a",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "- Customer - Customer ID\n",
    "\n",
    "- ST - State where customers live\n",
    "\n",
    "- Gender - Gender of the customer\n",
    "\n",
    "- Education - Background education of customers \n",
    "\n",
    "- Customer Lifetime Value - Customer lifetime value(CLV) is the total revenue the client will derive from their entire relationship with a customer. In other words, is the predicted or calculated value of a customer over their entire duration as a policyholder with the insurance company. It is an estimation of the net profit that the insurance company expects to generate from a customer throughout their relationship with the company. Customer Lifetime Value takes into account factors such as the duration of the customer's policy, premium payments, claim history, renewal likelihood, and potential additional services or products the customer may purchase. It helps insurers assess the long-term profitability and value associated with retaining a particular customer.\n",
    "\n",
    "- Income - Customers income\n",
    "\n",
    "- Monthly Premium Auto - Amount of money the customer pays on a monthly basis as a premium for their auto insurance coverage. It represents the recurring cost that the insured person must pay to maintain their insurance policy and receive coverage for potential damages, accidents, or other covered events related to their vehicle.\n",
    "\n",
    "- Number of Open Complaints - Number of complaints the customer opened\n",
    "\n",
    "- Policy Type - There are three type of policies in car insurance (Corporate Auto, Personal Auto, and Special Auto)\n",
    "\n",
    "- Vehicle Class - Type of vehicle classes that customers have Two-Door Car, Four-Door Car SUV, Luxury SUV, Sports Car, and Luxury Car\n",
    "\n",
    "- Total Claim Amount - the sum of all claims made by the customer. It represents the total monetary value of all approved claims for incidents such as accidents, theft, vandalism, or other covered events.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72419b-20fc-4905-817a-8c83abc59de6",
   "metadata": {},
   "source": [
    "External Resources: https://towardsdatascience.com/filtering-data-frames-in-pandas-b570b1f834b9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ece17-e919-4e23-96c0-c7c59778436a",
   "metadata": {},
   "source": [
    "## Challenge 1: Understanding the data\n",
    "\n",
    "In this challenge, you will use pandas to explore a given dataset. Your task is to gain a deep understanding of the data by analyzing its characteristics, dimensions, and statistical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91437bd5-59a6-49c0-8150-ef0e6e6eb253",
   "metadata": {},
   "source": [
    "- Identify the dimensions of the dataset by determining the number of rows and columns it contains.\n",
    "- Determine the data types of each column and evaluate whether they are appropriate for the nature of the variable. You should also provide suggestions for fixing any incorrect data types.\n",
    "- Identify the number of unique values for each column and determine which columns appear to be categorical. You should also describe the unique values of each categorical column and the range of values for numerical columns, and give your insights.\n",
    "- Compute summary statistics such as mean, median, mode, standard deviation, and quartiles to understand the central tendency and distribution of the data for numerical columns. You should also provide your conclusions based on these summary statistics.\n",
    "- Compute summary statistics for categorical columns and providing your conclusions based on these statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd4e8cd8-a6f6-486c-a5c4-1745b0c035f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Customer          ST GENDER             Education Customer Lifetime Value  \\\n",
      "0  RB50392  Washington    NaN                Master                     NaN   \n",
      "1  QZ44356     Arizona      F              Bachelor              697953.59%   \n",
      "2  AI49188      Nevada      F              Bachelor             1288743.17%   \n",
      "3  WW63253  California      M              Bachelor              764586.18%   \n",
      "4  GA49547  Washington      M  High School or Below              536307.65%   \n",
      "\n",
      "    Income  Monthly Premium Auto Number of Open Complaints     Policy Type  \\\n",
      "0      0.0                1000.0                    1/0/00   Personal Auto   \n",
      "1      0.0                  94.0                    1/0/00   Personal Auto   \n",
      "2  48767.0                 108.0                    1/0/00   Personal Auto   \n",
      "3      0.0                 106.0                    1/0/00  Corporate Auto   \n",
      "4  36357.0                  68.0                    1/0/00   Personal Auto   \n",
      "\n",
      "   Vehicle Class  Total Claim Amount  \n",
      "0  Four-Door Car            2.704934  \n",
      "1  Four-Door Car         1131.464935  \n",
      "2   Two-Door Car          566.472247  \n",
      "3            SUV          529.881344  \n",
      "4  Four-Door Car           17.269323  \n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# set-up, load the dataset\n",
    "\n",
    "# dataset location:  https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv\"\n",
    "data = pd.read_csv(url)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b380c876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions:\n",
      "(4008, 11)\n"
     ]
    }
   ],
   "source": [
    "# Identify the dimensions of the dataset \n",
    "# by determining the number of rows and columns it contains.\n",
    "print(\"Dataset dimensions:\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bdd548",
   "metadata": {},
   "source": [
    "> **Conclusion Dataset Dimensions:**\n",
    ">\n",
    "> The dataset contains 4,008 rows and 11 columns, which represents a medium-sized dataset.\n",
    ">\n",
    "> From a purely quantitative perspective, this size is appropriate for exploratory data analysis using pandas and does not present performance limitations.\n",
    ">\n",
    "> In principle, the number of rows appears sufficient to extract meaningful statistical insights and identify patterns. However, the reliability of these insights strongly depends on data quality. Issues such as inconsistent categorical values, missing data, incorrect data types, or outliers may reduce the validity of the results. Therefore, data cleaning and validation are necessary steps before drawing definitive conclusions or using the data for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a67728d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types of each column:\n",
      "Customer                      object\n",
      "ST                            object\n",
      "GENDER                        object\n",
      "Education                     object\n",
      "Customer Lifetime Value       object\n",
      "Income                       float64\n",
      "Monthly Premium Auto         float64\n",
      "Number of Open Complaints     object\n",
      "Policy Type                   object\n",
      "Vehicle Class                 object\n",
      "Total Claim Amount           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Determine the data types of each column \n",
    "# and evaluate whether they are appropriate for the nature of the variable. \n",
    "\n",
    "print(\"\\nData types of each column:\")\n",
    "print(data.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbd216f",
   "metadata": {},
   "source": [
    "> **Datatyppe Evaluation:**\n",
    "> \n",
    "> Based on the output of data.dtypes, most columns have data types that generally align with the nature of the variables. However, several important issues were identified.\n",
    ">\n",
    "> **1. Appropriate data types:**\n",
    ">\n",
    "> **Customer (object):**\n",
    " The data type is appropriate, as this column represents a unique customer identifier and should be treated as a categorical variable rather than a numerical one.\n",
    ">\n",
    "> **ST (object):**\n",
    " The data type is appropriate because this column represents categorical information (customer state).\n",
    ">\n",
    "> **GENDER (object):**\n",
    " The data type is appropriate for a categorical variable and gender is a categorical variable.\n",
    ">\n",
    "> **Education (object):**\n",
    " The data type is appropriate since education level is a categorical variable.\n",
    ">\n",
    "> **Policy Type (object):**\n",
    " The data type is appropriate because it represents distinct policy categories.\n",
    ">\n",
    "> **Vehicle Class (object):**\n",
    " The data type is appropriate, as this variable represents different vehicle categories.\n",
    ">\n",
    "> **Income (float64):**\n",
    " The data type is appropriate for a continuous numerical variable and Income is a continous numerical variable.\n",
    ">\n",
    "> **Monthly Premium Auto (float64):**\n",
    " The data type is appropriate for a numerical variable representing a monetary value.\n",
    ">\n",
    "> **Total Claim Amount (float64):**\n",
    " The data type is appropriate for a numerical variable representing monetary amounts.\n",
    ">\n",
    ">\n",
    "> **2. Data types requiring correction or clarification:**\n",
    ">\n",
    "> **Customer Lifetime Value (object):**\n",
    " This data type is not appropriate for the nature of the variable. Customer Lifetime Value is a numerical measure, but it is stored as an object due to the presence of percentage symbols (%) as we could see in our first output when we seted up and loaded the dataset.\n",
    ">\n",
    "> *Recommendation:* Remove the % symbol and convert this column to a numerical type (float) to enable proper quantitative analysis.\n",
    ">\n",
    "> **Number of Open Complaints (object):**\n",
    " This data type is potentially inappropriate. Although stored as an object, the values appear to encode numerical counts using a date-like format (e.g., 1/0/00).\n",
    " >\n",
    " > *Recommendation:* Clarify the semantic meaning of this column and convert it to an integer representing the number of complaints.\n",
    ">\n",
    ">\n",
    "> **Overall assessment:**\n",
    "> In summary, most variables are stored using appropriate data types. However, two columns require transformation to ensure correct interpretation and analysis. Addressing these issues is essential to improve data quality and avoid misleading analytical results.\n",
    ">\n",
    "> **Categorical variables** (Customer, ST, GENDER, Education, Policy Type, Vehicle Class)\n",
    " → Are appropiarly stored as object in the DataFrame.\n",
    ">\n",
    "> **Numerical variables** (Income, Monthly Premium Auto, Total Claim Amount)\n",
    " → Are also correctly stored as float64.\n",
    ">\n",
    "> **Problematic variables:**\n",
    ">\n",
    "> *Customer Lifetime Value:* this variable should be numerical, but it is stored as object. This need to be converted to a float.\n",
    ">\n",
    "> *Number of Open Complaints:* this variable represents a discrete numerical count, but it is stored as object. This need to be converted to an interger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e948ed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values and ranges for each column:\n",
      "\n",
      "Column: Customer\n",
      "Number of unique values: 1071\n",
      "Categorical values:\n",
      "Customer\n",
      "MY31220    1\n",
      "RB50392    1\n",
      "QZ44356    1\n",
      "AI49188    1\n",
      "WW63253    1\n",
      "          ..\n",
      "FL50705    1\n",
      "WC83389    1\n",
      "OE15005    1\n",
      "FV94802    1\n",
      "BW63560    1\n",
      "Name: count, Length: 1071, dtype: int64\n",
      "\n",
      "Column: ST\n",
      "Number of unique values: 8\n",
      "Categorical values:\n",
      "ST\n",
      "Oregon        320\n",
      "California    211\n",
      "Arizona       186\n",
      "Cali          120\n",
      "Nevada         98\n",
      "Washington     81\n",
      "WA             30\n",
      "AZ             25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: GENDER\n",
      "Number of unique values: 5\n",
      "Categorical values:\n",
      "GENDER\n",
      "F         457\n",
      "M         413\n",
      "Male       39\n",
      "female     28\n",
      "Femal      17\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: Education\n",
      "Number of unique values: 6\n",
      "Categorical values:\n",
      "Education\n",
      "Bachelor                324\n",
      "College                 313\n",
      "High School or Below    296\n",
      "Master                   94\n",
      "Doctor                   37\n",
      "Bachelors                 7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: Customer Lifetime Value\n",
      "Number of unique values: 1027\n",
      "Categorical values:\n",
      "Customer Lifetime Value\n",
      "251459.20%    4\n",
      "445811.34%    4\n",
      "578018.22%    3\n",
      "684615.03%    3\n",
      "477294.38%    3\n",
      "             ..\n",
      "251753.36%    1\n",
      "532667.77%    1\n",
      "260027.21%    1\n",
      "853479.28%    1\n",
      "882883.50%    1\n",
      "Name: count, Length: 1027, dtype: int64\n",
      "\n",
      "Column: Income\n",
      "Number of unique values: 774\n",
      "Numerical range:\n",
      "Min: 0.0, Max: 99960.0\n",
      "\n",
      "Column: Monthly Premium Auto\n",
      "Number of unique values: 132\n",
      "Numerical range:\n",
      "Min: 61.0, Max: 35354.0\n",
      "\n",
      "Column: Number of Open Complaints\n",
      "Number of unique values: 6\n",
      "Categorical values:\n",
      "Number of Open Complaints\n",
      "1/0/00    830\n",
      "1/1/00    138\n",
      "1/2/00     50\n",
      "1/3/00     34\n",
      "1/4/00     13\n",
      "1/5/00      6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: Policy Type\n",
      "Number of unique values: 3\n",
      "Categorical values:\n",
      "Policy Type\n",
      "Personal Auto     780\n",
      "Corporate Auto    234\n",
      "Special Auto       57\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: Vehicle Class\n",
      "Number of unique values: 6\n",
      "Categorical values:\n",
      "Vehicle Class\n",
      "Four-Door Car    576\n",
      "Two-Door Car     205\n",
      "SUV              199\n",
      "Sports Car        57\n",
      "Luxury SUV        20\n",
      "Luxury Car        14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: Total Claim Amount\n",
      "Number of unique values: 761\n",
      "Numerical range:\n",
      "Min: 0.382107, Max: 2893.239678\n"
     ]
    }
   ],
   "source": [
    "# Identify the number of unique values for each column \n",
    "# and determine which columns appear to be categorical. \n",
    "# You should also describe the unique values of each categorical column \n",
    "# and the range of values for numerical columns, and give your insights.\n",
    "\n",
    "print(\"\\nUnique values and ranges for each column:\")\n",
    "for column in data.columns:\n",
    "    unique_values = data[column].nunique()\n",
    "    print(f\"\\nColumn: {column}\")\n",
    "    print(f\"Number of unique values: {unique_values}\")\n",
    "    if data[column].dtype == 'object':\n",
    "        print(\"Categorical values:\")\n",
    "        print(data[column].value_counts())\n",
    "    else:\n",
    "        print(\"Numerical range:\")\n",
    "        print(f\"Min: {data[column].min()}, Max: {data[column].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6839155",
   "metadata": {},
   "source": [
    "> **Evaluation of Value Consistency and Data Quality:**\n",
    ">\n",
    "> This section evaluates the consistency of values across all variables based on their unique values and distributions. For each variable, potential inconsistencies are identified, their impact on data quality is assessed, and recommendations are provided where necessary.\n",
    ">\n",
    "> **1. Customer:**\n",
    ">\n",
    "> *Evaluation of consistency*\n",
    "> The column contains 1,071 unique values, each appearing exactly once.\n",
    "> This behavior is consistent with a unique customer identifier.\n",
    ">\n",
    "> *Impact on data quality*\n",
    "> No negative impact. The variable behaves as expected.\n",
    ">\n",
    "> *Recommendation*\n",
    "> No action required. The column should be treated as an identifier and excluded from aggregations or statistical analysis.\n",
    ">\n",
    ">\n",
    "> **2. State (ST):**\n",
    ">\n",
    "> *Evaluation of consistency*\n",
    "> The column contains 8 unique values.\n",
    "> \n",
    "> Multiple representations exist for the same state:\n",
    "> - California vs Cali\n",
    "> - Washington vs WA\n",
    "> - Arizona vs AZ\n",
    ">\n",
    "> *Inconsistency identified*\n",
    "> Semantic duplication caused by inconsistent naming conventions.\n",
    "> \n",
    "> *Impact on data quality*\n",
    "> - This fragmentation leads to incorrect aggregations and misleading geographic insights.\n",
    "> - State-level analyses may underestimate or overestimate customer counts.\n",
    ">\n",
    "> *Recommendation*\n",
    "> - Standardize state names using a consistent format (e.g. full state names).\n",
    "> - Apply a mapping dictionary to normalize values.\n",
    ">\n",
    ">\n",
    "> **3. Gender:**\n",
    ">\n",
    "> *Evaluation of consistency*\n",
    "> The column contains 5 unique values representing only two genders.\n",
    "> \n",
    "> Inconsistencies include:\n",
    "> - Abbreviations (F, M)\n",
    "> - Full words (Male, female)\n",
    "> - Typographical errors (Femal)\n",
    "> - Inconsistency identified\n",
    "> - Mixed formats and spelling errors.\n",
    ">\n",
    "> *Impact on data quality*\n",
    "> - Gender-based analysis becomes unreliable.\n",
    "> - Grouping operations may produce fragmented or incorrect results.\n",
    ">\n",
    "> *Recommendation*\n",
    "> - Normalize values into a standardized set (e.g. Male, Female).\n",
    "> - Optionally handle missing or ambiguous values explicitly.\n",
    ">\n",
    ">\n",
    "> **4. Education:**\n",
    ">\n",
    "> *Evaluation of consistency*\n",
    "> The column contains 6 unique values.\n",
    ">\n",
    "> Minor duplication exists:\n",
    "> - Bachelor vs Bachelors\n",
    ">\n",
    "> *Inconsistency identified*\n",
    "> - Semantic duplication of equivalent education levels.\n",
    ">\n",
    "> *Impact on data quality*\n",
    "> - Slight distortion in education-level distributions.\n",
    "> - Reduced clarity for segmentation analysis.\n",
    ">\n",
    "> *Recommendation*\n",
    "> - Merge equivalent categories into a single standardized value.\n",
    "> - Optionally define an ordinal hierarchy if used for modeling.\n",
    ">\n",
    ">\n",
    "> **5. Customer Lifetime Value:**\n",
    ">\n",
    "> *Evaluation of consistency*\n",
    "> - The column contains 1,027 unique values, indicating a continuous variable.\n",
    "> - Values are stored as strings with a percentage symbol (%).\n",
    ">\n",
    "> *Inconsistency identified*\n",
    "> - Numerical values stored as text.\n",
    "> - Misclassification as categorical data.\n",
    "> \n",
    "> *Impact on data quality*\n",
    "> - Prevents numerical analysis, aggregation, and modeling.\n",
    "> - Increases risk of misinterpretation.\n",
    ">\n",
    "> *Recommendation*\n",
    "> - Remove the % symbol and convert the column to a numerical type (float).\n",
    "> - Treat this variable as continuous rather than categorical.\n",
    ">\n",
    ">\n",
    "> **6. Income:**\n",
    ">\n",
    "> *Evaluation of consistency*\n",
    "> Contains 774 unique values with a range from 0 to 99,960.\n",
    "> Distribution appears plausible for income data.\n",
    ">\n",
    "> *Potential issue identified*\n",
    "> - A significant number of zero values may represent missing or default entries.\n",
    ">\n",
    "> *Impact on data quality*\n",
    "> - ero values may bias average income calculations.\n",
    ">\n",
    "> *Recommendation*\n",
    "> - Investigate whether zero income represents missing data.\n",
    "> - Consider replacing invalid zeros with NaN if appropriate.\n",
    ">\n",
    ">\n",
    "> **7. Monthly Premium Auto**\n",
    ">\n",
    "> *Evaluation of consistency*\n",
    "> Contains 132 unique values.\n",
    "> Range from 61 to 35,354.\n",
    ">\n",
    "> *Potential issue identified*\n",
    "> - Presence of extreme values suggests outliers.\n",
    ">\n",
    "> *Impact on data quality*\n",
    "> Outliers may skew averages and standard deviation.\n",
    "> \n",
    "> *Recommendation*\n",
    "> - Investigate unusually high premiums.\n",
    "> - Consider outlier treatment (capping or filtering) depending on analysis goals.\n",
    ">\n",
    ">\n",
    "> **8. Number of Open Complaints:**\n",
    ">\n",
    "> *Evaluation of consistency*\n",
    "> Contains 6 unique values, all encoded in a date-like format (1/0/00, 1/1/00, etc.).\n",
    ">\n",
    "> *Inconsistency identified*\n",
    "> Complaint counts are stored using an ambiguous and misleading format.\n",
    ">\n",
    "> *Impact on data quality*\n",
    "> - High risk of misinterpretation.\n",
    "> - Limits usability for numerical analysis.\n",
    ">\n",
    "> *Recommendation*\n",
    "> - Extract the numerical complaint count.\n",
    "> - Convert the column to an integer type representing the number of complaints.\n",
    ">\n",
    "> \n",
    "> **9. Policy Type:**\n",
    ">\n",
    "> *Evaluation of consistency*\n",
    "> Contains 3 unique values.\n",
    "> Categories are clearly defined and consistently labeled.\n",
    "> \n",
    "> *Impact on data quality*\n",
    "> No issues identified.\n",
    ">\n",
    "> *Recommendation*\n",
    "> No action required.\n",
    ">\n",
    "> **10. Vehicle Class:**\n",
    ">\n",
    "> *Evaluation of consistency*\n",
    "> Contains 6 unique values.\n",
    "> Categories are well defined and internally consistent.\n",
    ">\n",
    "> *Impact on data quality*\n",
    "> No issues identified.\n",
    ">\n",
    "> *Recommendation*\n",
    "> No action required.\n",
    ">\n",
    ">\n",
    "> **11. Total Claim Amount**\n",
    ">\n",
    "> *Evaluation of consistency*\n",
    "> Contains 761 unique values.\n",
    "> Range appears realistic for insurance claim amounts.\n",
    "> \n",
    "> *Impact on data quality*\n",
    "> No structural issues identified.\n",
    ">\n",
    "> *Recommendation*\n",
    "> - Variable is suitable for numerical analysis.\n",
    "> - Consider outlier analysis in later stages.\n",
    ">\n",
    "> \n",
    "> **Overall Assessment:**\n",
    ">\n",
    "> While the dataset contains a sufficient number of observations, several value consistency issues were identified, primarily affecting categorical variables and incorrectly typed numerical fields. These issues can significantly reduce the reliability of aggregated insights and analytical results. Addressing these inconsistencies through data cleaning and standardization is a necessary step before performing advanced analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18a0b935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics for numerical columns:\n",
      "             Income  Monthly Premium Auto  Total Claim Amount\n",
      "count   1071.000000           1071.000000         1071.000000\n",
      "mean   39295.701214            193.234360          404.986909\n",
      "std    30469.427060           1601.190369          293.027260\n",
      "min        0.000000             61.000000            0.382107\n",
      "25%    14072.000000             68.000000          202.157702\n",
      "50%    36234.000000             83.000000          354.729129\n",
      "75%    64631.000000            109.500000          532.800000\n",
      "max    99960.000000          35354.000000         2893.239678\n"
     ]
    }
   ],
   "source": [
    "#  Compute summary statistics such as mean, median, mode, standard deviation, \n",
    "# and quartiles to understand the central tendency \n",
    "# and distribution of the data for numerical columns. \n",
    "# You should also provide your conclusions based on these summary statistics.\n",
    "print(\"\\nSummary statistics for numerical columns:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c73fd0d",
   "metadata": {},
   "source": [
    "> **Conclusions Based on Summary Statistics:**\n",
    ">\n",
    "> The summary statistics provide insights into the central tendency, dispersion, and distribution of the numerical variables in the dataset: Income, Monthly Premium Auto, and Total Claim Amount.\n",
    ">\n",
    "> \n",
    "> **1. Income:**\n",
    "> - The mean income is approximately 39,295.70, while the median is 36,234.00.\n",
    "> The mean being higher than the median suggests a right-skewed distribution, where higher-income values pull the average upward.\n",
    "> - The minimum income is 0, indicating that some customers report no income.\n",
    "> The wide range (0 to 99,960) and high standard deviation (30,469.43) suggest substantial variability in customer income levels.\n",
    ">\n",
    "> *Conclusion:*\n",
    "> Income is unevenly distributed across customers, with significant dispersion and potential outliers. The presence of zero-income values may indicate missing data or special customer categories, which could bias income-based analyses if not addressed.\n",
    ">\n",
    ">\n",
    "> **2. Monthly Premium Auto:**\n",
    "> - The median monthly premium is 83.00, while the mean is considerably higher at 193.23.\n",
    "> This large difference between the mean and the median indicates a strong right-skewed distribution.\n",
    "> - The maximum value (35,354) is extremely high relative to the upper quartile (109.50), suggesting the presence of extreme outliers.\n",
    "> The standard deviation (1,601.19) is disproportionately large compared to the interquartile range.\n",
    ">\n",
    "> *Conclusion:*\n",
    "Most customers pay relatively low monthly premiums, while a small number of extreme values heavily distort the average. Outlier analysis is strongly recommended before using this variable for modeling or business decisions.\n",
    ">\n",
    "> \n",
    "> **3. Total Claim Amount:**\n",
    "> - The mean claim amount is approximately 404.99, while the median is 354.73.\n",
    "> The distribution is moderately right-skewed, with higher claim values affecting the mean.\n",
    "> - The interquartile range (approximately 202.16 to 532.80) shows that most customers fall within a relatively narrow claim range.\n",
    "> The maximum value (2,893.24) indicates the presence of high-claim customers.\n",
    ">\n",
    "> *Conclusion:*\n",
    "Most customers generate relatively low to moderate claim amounts, while a smaller group accounts for significantly higher claims. This variable is well suited for identifying high-risk or high-cost customer segments.\n",
    ">\n",
    "> \n",
    "> **Overall Conclusions**\n",
    "> - All numerical variables exhibit right-skewed distributions, indicating the presence of outliers.\n",
    "> - The median provides a more representative measure of central tendency than the mean for all variables.\n",
    "> - High variability in income and premium values suggests heterogeneous customer profiles.\n",
    "> - Before conducting advanced analysis or predictive modeling, outlier treatment and data validation are recommended to improve the robustness of insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1543312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics for categorical columns:\n",
      "\n",
      "Column: Customer\n",
      "count        1071\n",
      "unique       1071\n",
      "top       MY31220\n",
      "freq            1\n",
      "Name: Customer, dtype: object\n",
      "\n",
      "Column: ST\n",
      "count       1071\n",
      "unique         8\n",
      "top       Oregon\n",
      "freq         320\n",
      "Name: ST, dtype: object\n",
      "\n",
      "Column: GENDER\n",
      "count     954\n",
      "unique      5\n",
      "top         F\n",
      "freq      457\n",
      "Name: GENDER, dtype: object\n",
      "\n",
      "Column: Education\n",
      "count         1071\n",
      "unique           6\n",
      "top       Bachelor\n",
      "freq           324\n",
      "Name: Education, dtype: object\n",
      "\n",
      "Column: Customer Lifetime Value\n",
      "count           1068\n",
      "unique          1027\n",
      "top       251459.20%\n",
      "freq               4\n",
      "Name: Customer Lifetime Value, dtype: object\n",
      "\n",
      "Column: Number of Open Complaints\n",
      "count       1071\n",
      "unique         6\n",
      "top       1/0/00\n",
      "freq         830\n",
      "Name: Number of Open Complaints, dtype: object\n",
      "\n",
      "Column: Policy Type\n",
      "count              1071\n",
      "unique                3\n",
      "top       Personal Auto\n",
      "freq                780\n",
      "Name: Policy Type, dtype: object\n",
      "\n",
      "Column: Vehicle Class\n",
      "count              1071\n",
      "unique                6\n",
      "top       Four-Door Car\n",
      "freq                576\n",
      "Name: Vehicle Class, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Compute summary statistics for categorical columns \n",
    "# and providing your conclusions based on these statistics.\n",
    "\n",
    "print(\"\\nSummary statistics for categorical columns:\")\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    print(f\"\\nColumn: {column}\")\n",
    "    print(data[column].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf38125",
   "metadata": {},
   "source": [
    "> **Conclusions Based on Summary Statistics for Categorical Variables:**\n",
    ">\n",
    "> The summary statistics for the categorical variables provide insights into data completeness, category dominance, and potential data quality issues.\n",
    ">\n",
    "> \n",
    "> **1. Customer:**\n",
    "> - The column contains 1,071 non-null values, all of which are unique.\n",
    "> - The most frequent value appears only once.\n",
    ">\n",
    "> *Conclusion:*\n",
    "> This column functions purely as a unique identifier. It does not provide analytical value for aggregation or statistical analysis and should be excluded from modeling and descriptive summaries.\n",
    ">\n",
    "> \n",
    "> **2. State (ST):**\n",
    "> - The column contains 1,071 non-null values and 8 unique categories.\n",
    "> - Oregon is the most frequent state, with 320 occurrences.\n",
    ">\n",
    "> *Conclusion:*\n",
    "> State distribution is uneven, with a clear dominance of certain locations. However, the presence of multiple representations for the same state (e.g., abbreviations and full names) indicates inconsistent categorical encoding, which may distort geographic analyses.\n",
    ">\n",
    "> \n",
    "> **3. Gender:**\n",
    "> - The column contains 954 non-null values, indicating missing data.\n",
    "> - There are 5 unique categories, with F being the most frequent value.\n",
    ">\n",
    "> *Conclusion:*\n",
    "> Gender data is incomplete and inconsistently encoded. The presence of multiple labels for the same gender reduces reliability and complicates demographic analysis. Standardization and missing value handling are necessary before further use.\n",
    ">\n",
    ">\n",
    "> **4. Education:**\n",
    "> - The column contains 1,071 non-null values and 6 unique categories.\n",
    "> - Bachelor is the most common education level.\n",
    "> \n",
    "> *Conclusion:*\n",
    "> Education data is largely complete and informative, but minor inconsistencies (e.g., Bachelor vs Bachelors) may slightly distort category distributions. Consolidation of equivalent categories is recommended.\n",
    ">\n",
    ">\n",
    "> **5. Customer Lifetime Value:**\n",
    "> - The column contains 1,068 non-null values, indicating some missing values.\n",
    "> - There are 1,027 unique values, which is unusually high for a categorical variable.\n",
    ">\n",
    "> *Conclusion:*\n",
    "> Although currently treated as categorical, this variable behaves like a continuous numerical measure. Storing it as text prevents meaningful statistical analysis. Conversion to a numerical format is essential to unlock its analytical value.\n",
    ">\n",
    ">\n",
    "> **6. Number of Open Complaints**\n",
    "> - The column contains 1,071 non-null values and 6 unique values.\n",
    "> - The most frequent value (1/0/00) appears 830 times.\n",
    ">\n",
    "> *Conclusion:*\n",
    "> This variable shows a highly skewed distribution, with most customers reporting zero complaints. However, the date-like encoding is misleading and may cause misinterpretation. Re-encoding this variable as an integer count is strongly recommended.\n",
    ">\n",
    "> \n",
    "> **7. Policy Type:**\n",
    "> - The column contains 1,071 non-null values and 3 unique categories.\n",
    "> - Personal Auto is the dominant policy type.\n",
    "> \n",
    "> *Conclusion:*\n",
    "> Policy type data is complete and consistently encoded. However, the dataset is heavily skewed toward Personal Auto policies, which may bias analyses and conclusions.\n",
    ">\n",
    ">\n",
    "> **8. Vehicle Class:**\n",
    "> - The column contains 1,071 non-null values and 6 unique categories.\n",
    "> - Four-Door Car is the most frequent vehicle class.\n",
    ">\n",
    "> *Conclusion:*\n",
    "> Vehicle class data is complete and well structured. The distribution reflects realistic customer preferences and is suitable for segmentation and risk analysis.\n",
    ">\n",
    ">\n",
    "> **Overall Conclusions for Categorical Variables:**\n",
    "> - Most categorical variables are complete, but several suffer from inconsistent encoding.\n",
    "> - Identifier variables should be excluded from analysis.\n",
    "> - Some variables currently treated as categorical should be reclassified or transformed.\n",
    "> - Data cleaning and standardization are necessary steps to improve analytical reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbdfc54",
   "metadata": {},
   "source": [
    "This exploratory analysis focused on understanding the structure, distributions, and quality of the data. Several inconsistencies and data type issues were identified; however, no transformations were applied at this stage, as the objective of this challenge is limited to analysis and recommendations. The identified issues provide clear guidance for future data cleaning and preparation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a703890-63db-4944-b7ab-95a4f8185120",
   "metadata": {},
   "source": [
    "## Challenge 2: analyzing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776a403-c56a-452f-ac33-5fd4fdb06fc7",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedbc484-da4d-4f9c-9343-e1d44311a87e",
   "metadata": {},
   "source": [
    "The marketing team wants to know the top 5 less common customer locations. Create a pandas Series object that contains the customer locations and their frequencies, and then retrieve the top 5 less common locations in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2dca5073-4520-4f42-9390-4b92733284ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 less common customer locations:\n",
      "ST\n",
      "AZ             25\n",
      "WA             30\n",
      "Washington     81\n",
      "Nevada         98\n",
      "Cali          120\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# The marketing team wants to know the top 5 less common customer locations.\n",
    "# Create a pandas Series object that contains the customer locations and their frequencies\n",
    "# sorted in ascending order. Display the top 5 less common customer locations.\n",
    "\n",
    "customer_location_counts = data['ST'].value_counts(ascending=True)\n",
    "top_5_less_common_locations = customer_location_counts.head(5)\n",
    "print(\"\\nTop 5 less common customer locations:\")\n",
    "print(top_5_less_common_locations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15960f",
   "metadata": {},
   "source": [
    "> **Note on data consistency and interpretation:**\n",
    ">\n",
    ">\n",
    "> The results above are derived directly from the raw dataset, without applying any data cleaning or standardization to the ST (state) variable. As identified in the exploratory analysis, this column contains inconsistencies such as multiple representations of the same state (e.g., abbreviations and full names).\n",
    ">\n",
    "> At this stage, the analysis intentionally uses the data as is, in accordance with the objective of this exercise.\n",
    ">\n",
    ">\n",
    "> *Evaluation and potential risk*\n",
    ">\n",
    "> While the result correctly identifies the five least common customer locations based on the raw data, these inconsistencies may lead to fragmented counts and misleading geographic insights. States represented under different labels may appear less frequent than they actually are when aggregated correctly.\n",
    "Therefore, conclusions drawn from this result should be interpreted with caution. For accurate location-based analysis and decision-making, value standardization should be applied in a subsequent data cleaning phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce80f43-4afa-43c7-a78a-c917444da4e0",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "The sales team wants to know the total number of policies sold for each type of policy. Create a pandas Series object that contains the policy types and their total number of policies sold, and then retrieve the policy type with the highest number of policies sold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f13997-1555-4f98-aca6-970fda1d2c3f",
   "metadata": {},
   "source": [
    "*Hint:*\n",
    "- *Using value_counts() method simplifies this analysis.*\n",
    "- *Futhermore, there is a method that returns the index of the maximum value in a column or row.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcfad6c1-9af2-4b0b-9aa9-0dc5c17473c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of policies sold for each type of policy:\n",
      "Policy Type\n",
      "Personal Auto     780\n",
      "Corporate Auto    234\n",
      "Special Auto       57\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Policy type with the highest number of policies sold: Personal Auto\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# The sales team wants to know the total number of policies sold for each type of policy.\n",
    "# Create a pandas Series object that contains the policy types and their total number of policies sold\n",
    "# and then retrieve the policy type with the highest number of policies sold.\n",
    "# Using value_counts() method simplifies this analysis\n",
    "# Futhermore, there is a method that returns the index of the maximum value in a column or row\n",
    "\n",
    "policy_type_counts = data['Policy Type'].value_counts()\n",
    "most_sold_policy_type = policy_type_counts.idxmax()\n",
    "print(\"\\nTotal number of policies sold for each type of policy:\")\n",
    "print(policy_type_counts)\n",
    "print(f\"\\nPolicy type with the highest number of policies sold: {most_sold_policy_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73697aaa",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    ">\n",
    "> The results show that Personal Auto is by far the most common policy type, accounting for the majority of policies sold. Corporate Auto and Special Auto policies represent a significantly smaller portion of the dataset.\n",
    ">\n",
    "> Since the Policy Type variable is consistently encoded and does not present data quality issues, these results can be considered reliable. However, the strong dominance of Personal Auto policies indicates an imbalanced distribution, which should be taken into account when performing comparative analyses across policy types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b863fd3-bf91-4d5d-86eb-be29ed9f5b70",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "The sales team wants to know if customers with Personal Auto have a lower income than those with Corporate Auto. How does the average income compare between the two policy types?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1386d75-2810-4aa1-93e0-9485aa12d552",
   "metadata": {},
   "source": [
    "- Use *loc* to create two dataframes: one containing only Personal Auto policies and one containing only Corporate Auto policies.\n",
    "- Calculate the average income for each policy.\n",
    "- Print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c0563cf-6f8b-463d-a321-651a972f82e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average income for Personal Auto customers: 38180.69871794872\n",
      "Average income for Corporate Auto customers: 41390.31196581197\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# The sales team wants to know if customers with Personal Auto have a lower income than those with Corporate Auto.\n",
    "# How does the average income compare between the two policy types?\n",
    "# Use loc to create two dataframes: one containing only Personal Auto policies and one containing only Corporate Auto policies.\n",
    "# Calculate the average income for each policy.\n",
    "# Print the results.\n",
    "\n",
    "personal_auto = data.loc[data['Policy Type'] == 'Personal Auto']\n",
    "corporate_auto = data.loc[data['Policy Type'] == 'Corporate Auto']\n",
    "avg_income_personal = personal_auto['Income'].mean()\n",
    "avg_income_corporate = corporate_auto['Income'].mean()\n",
    "print(f\"\\nAverage income for Personal Auto customers: {avg_income_personal}\")\n",
    "print(f\"Average income for Corporate Auto customers: {avg_income_corporate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cec276",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    ">\n",
    "> The results indicate that customers with Corporate Auto policies have a higher average income than those with Personal Auto policies. This suggests that corporate policyholders tend to belong to higher-income segments. However, this comparison is based on raw data and does not account for potential outliers or zero-income values, which may influence the average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b16c27-f4a5-4727-a229-1f88671cf4e2",
   "metadata": {},
   "source": [
    "### Bonus: Exercise 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac584986-299b-475f-ac2e-928c16c3f512",
   "metadata": {},
   "source": [
    "Your goal is to identify customers with a high policy claim amount.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Review again the statistics for total claim amount to gain an understanding of the data.\n",
    "- To identify potential areas for improving customer retention and profitability, we want to focus on customers with a high policy claim amount. Consider customers with a high policy claim amount to be those in the top 25% of the total claim amount. Create a pandas DataFrame object that contains information about customers with a policy claim amount greater than the 75th percentile.\n",
    "- Use DataFrame methods to calculate summary statistics about the high policy claim amount data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3af5f1-6023-4b05-9c01-d05392daa650",
   "metadata": {},
   "source": [
    "*Note: When analyzing data, we often want to focus on certain groups of values to gain insights. Percentiles are a useful tool to help us define these groups. A percentile is a measure that tells us what percentage of values in a dataset are below a certain value. For example, the 75th percentile represents the value below which 75% of the data falls. Similarly, the 25th percentile represents the value below which 25% of the data falls. When we talk about the top 25%, we are referring to the values that fall above the 75th percentile, which represent the top quarter of the data. On the other hand, when we talk about the bottom 25%, we are referring to the values that fall below the 25th percentile, which represent the bottom quarter of the data. By focusing on these groups, we can identify patterns and trends that may be useful for making decisions and taking action.*\n",
    "\n",
    "*Hint: look for a method that gives you the percentile or quantile 0.75 and 0.25 for a Pandas Series.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d234634-50bd-41e0-88f7-d5ba684455d1",
   "metadata": {},
   "source": [
    "*Hint 2: check `Boolean selection according to the values of a single column` in https://towardsdatascience.com/filtering-data-frames-in-pandas-b570b1f834b9*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b731bca6-a760-4860-a27b-a33efa712ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Customers with high policy claim amounts (greater than 75th percentile):\n",
      "     Customer          ST GENDER Education Customer Lifetime Value   Income  \\\n",
      "1     QZ44356     Arizona      F  Bachelor              697953.59%      0.0   \n",
      "2     AI49188      Nevada      F  Bachelor             1288743.17%  48767.0   \n",
      "17    OE15005        Cali    NaN   College              394524.16%  28855.0   \n",
      "23    TZ98966      Nevada    NaN  Bachelor              245019.10%      0.0   \n",
      "26    US89481  California    NaN  Bachelor              394637.21%      0.0   \n",
      "...       ...         ...    ...       ...                     ...      ...   \n",
      "1059  YG44474      Oregon      M   College             1401472.13%  54193.0   \n",
      "1061  RY92647        Cali      F  Bachelor             1050677.17%      0.0   \n",
      "1068  GS98873     Arizona      F  Bachelor              323912.47%  16061.0   \n",
      "1069  CW49887  California      F    Master              462680.11%  79487.0   \n",
      "1070  MY31220  California      F   College              899704.02%  54230.0   \n",
      "\n",
      "      Monthly Premium Auto Number of Open Complaints     Policy Type  \\\n",
      "1                     94.0                    1/0/00   Personal Auto   \n",
      "2                    108.0                    1/0/00   Personal Auto   \n",
      "17                   101.0                    1/0/00   Personal Auto   \n",
      "23                    73.0                    1/3/00  Corporate Auto   \n",
      "26                   111.0                    1/0/00   Personal Auto   \n",
      "...                    ...                       ...             ...   \n",
      "1059                 117.0                    1/0/00  Corporate Auto   \n",
      "1061                  92.0                    1/0/00   Personal Auto   \n",
      "1068                  88.0                    1/0/00   Personal Auto   \n",
      "1069                 114.0                    1/0/00    Special Auto   \n",
      "1070                 112.0                    1/0/00   Personal Auto   \n",
      "\n",
      "      Vehicle Class  Total Claim Amount  \n",
      "1     Four-Door Car         1131.464935  \n",
      "2      Two-Door Car          566.472247  \n",
      "17              SUV          647.442031  \n",
      "23    Four-Door Car          554.376763  \n",
      "26    Four-Door Car          799.200000  \n",
      "...             ...                 ...  \n",
      "1059            SUV          720.752945  \n",
      "1061  Four-Door Car          546.524896  \n",
      "1068  Four-Door Car          633.600000  \n",
      "1069            SUV          547.200000  \n",
      "1070   Two-Door Car          537.600000  \n",
      "\n",
      "[264 rows x 11 columns]\n",
      "\n",
      "Summary statistics for high policy claim amounts:\n",
      "             Income  Monthly Premium Auto  Total Claim Amount\n",
      "count    264.000000            264.000000          264.000000\n",
      "mean   23677.344697            165.193182          782.228263\n",
      "std    27013.483721            623.930992          292.751640\n",
      "min        0.000000             63.000000          537.600000\n",
      "25%        0.000000             99.000000          606.521741\n",
      "50%    18807.000000            114.000000          679.597985\n",
      "75%    42423.750000            133.250000          851.400000\n",
      "max    99316.000000          10202.000000         2893.239678\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# Your goal is to identify customers with a high policy claim amount.\n",
    "# Review again the statistics for total claim amount to gain an understanding of the data.\n",
    "# To identify potential areas for improving customer retention and profitability, we want to focus on customers with a high policy claim amount.\n",
    "# Consider customers with a high policy claim amount to be those in the top 25% of the total claim amount.\n",
    "# Create a pandas DataFrame object that contains information about customers with a policy claim amount greater than the 75th percentile.\n",
    "# Use DataFrame methods to calculate summary statistics about the high policy claim amount data. \n",
    "# Use method that gives you the percentile or quantile 0.75 and 0.25 for a Pandas Series.\n",
    "\n",
    "quantile_75 = data['Total Claim Amount'].quantile(0.75)\n",
    "high_claims = data.loc[data['Total Claim Amount'] > quantile_75]\n",
    "print(\"\\nCustomers with high policy claim amounts (greater than 75th percentile):\")\n",
    "print(high_claims)\n",
    "print(\"\\nSummary statistics for high policy claim amounts:\")\n",
    "print(high_claims.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b57992",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    ">\n",
    "> The analysis above is based on the raw dataset. Previously identified data quality issues (e.g., inconsistent categorical values and numerical fields stored as text) have not been addressed at this stage. While the results correctly identify the top 25% of customers by claim amount, further data cleaning may be required before drawing operational or strategic conclusions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
